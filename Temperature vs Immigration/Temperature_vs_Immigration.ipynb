{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationship between Temperatures and Immigration\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "Im this work we are going to find the immigration beahavior between temperatures\n",
    "and immigration.\n",
    "\n",
    "We we will build a pipeline considering that you have a Huge amount of data and just\n",
    "you extract a certain amount of them to do the analsysis.\n",
    "\n",
    "We introduce a a new method that we are considering is the spliting any huge dataset\n",
    "into pieces and study them in the pipeline.\n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How do we use this data model to answer the immigration behavior?\n",
    "\n",
    "Well after get the table of the results immigration and temperatures\n",
    "we can plot the frequency of the temperatarures by ranges \n",
    "and identify which temperatures has more frequency and indentify which cities\n",
    "has these teperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import udf\n",
    "import psycopg2\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.version "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The scope of this project is to determine the relationship between immigration and temperatures\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "I94 Immigration Data: This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. This is where the data comes from. There's a sample file so you can take a look at the data in csv format before reading it all in. You do not have to use the entire dataset, just use what you need to accomplish the goal you set at the beginning of the project.\n",
    "\n",
    "\n",
    "World Temperature Data: This dataset came from Kaggle.\n",
    "GlobalLandTemperaturesByCity.csv\n",
    "\n",
    "In this dataset, we have include several files:\n",
    "\n",
    "Global Land and Ocean-and-Land Temperatures (GlobalTemperatures.csv):\n",
    "\n",
    "Date: starts in 1750 for average land temperature and 1850 for max and min land temperatures and global ocean and land temperatures\n",
    "LandAverageTemperature: global average land temperature in celsius\n",
    "LandAverageTemperatureUncertainty: the 95% confidence interval around the average\n",
    "LandMaxTemperature: global average maximum land temperature in celsius\n",
    "LandMaxTemperatureUncertainty: the 95% confidence interval around the maximum land temperature\n",
    "LandMinTemperature: global average minimum land temperature in celsius\n",
    "LandMinTemperatureUncertainty: the 95% confidence interval around the minimum land temperature\n",
    "LandAndOceanAverageTemperature: global average land and ocean temperature in celsius\n",
    "LandAndOceanAverageTemperatureUncertainty: the 95% confidence interval around the global average land and ocean temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "Large datasets always present an extra challenge that needs to be handled with care.\n",
    "You start to run into hardware roadblocks since you don’t have enough RAM to hold all the data in memory. \n",
    "\n",
    "Enterprise companies store datasets that get up to the range of 100s or even 1000s of GBs .\n",
    "\n",
    " This article will use one technique to reduce the memory footprint and read-in time for your large dataset. \n",
    " \n",
    "#### Chunking your data\n",
    "CSV format is a very convenient way to store data, being both easy to write to and human readable. When the CSV is so big that and we run out of memory instead trying to handle our data all at once, we’re going to do it in pieces. Typically, these pieces are referred to as chunks.\n",
    "A chunk is just a part of our dataset. We can make that chunk as big or as small as we want. It just depends on how much RAM we have.\n",
    "The process then works as follows:\n",
    "1. Read in a chunk Process the chunk\n",
    "2. Save the results of the chunks \n",
    "3. Merge all the chunk results\n",
    "\n",
    "We can perform all of the above steps using a handy variable of the read_csv() function called chunksize.\n",
    "The chunksize refers to how many CSV rows pandas will read at a time. This will of course depend on how much RAM you have and how big each row is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read April 2016 I94 immigration data as Udacity example\n",
    "immigration = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immigration dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ordering to deal with big data , sometimes you require a lot of memory, so to avoid this issue we can split the data by chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_large_data_sas(path,file):\n",
    "#    We create chunks of the big dataset\n",
    "# path : path where I want save the chunks\n",
    "# file : path of the large dataset\n",
    "    path_name=path+'chunk'\n",
    "    chunk_size=100000\n",
    "    batch_no=1\n",
    "    for chunk in pd.read_sas(file, encoding=\"ISO-8859-1\",chunksize=chunk_size):\n",
    "        chunk.to_csv(path_name+str(batch_no)+'.csv',index=False)\n",
    "        batch_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing a single chunk file into pandas dataframe:\n",
    "We now have multiple chunks, and each chunk can easily be loaded as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_large_data_sas(\"./data/\",immigration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count the number  of chunks created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fileCount(folder,extension):\n",
    "    import glob\n",
    "    count = len(glob.glob1(folder,\"*.\"+extension))\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We have created 31 chunks of Immigration data\n"
     ]
    }
   ],
   "source": [
    "csv_chunks=fileCount(\"./data/\",\"csv\")\n",
    "print(\" We have created \"+ str(csv_chunks) +\" chunks of Immigration data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration1 = pd.read_csv('./data/chunk1.csv')\n",
    "df_immigration1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the temperature data\n",
    "temperature_data = '../../data2/GlobalLandTemperaturesByCity.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_large_data_csv(path,file):\n",
    "# We create chunks of the big dataset\n",
    "# path : path where I want save the chunks\n",
    "# file : path of the large dataset\n",
    "#    We create chunks of the big dataset\n",
    "    path_name=path+'chunk'\n",
    "    chunk_size=100000\n",
    "    batch_no=1\n",
    "    for chunk in pd.read_csv(file,chunksize=chunk_size):\n",
    "        chunk.to_csv(path_name+str(batch_no)+'.csv',index=False)\n",
    "        batch_no+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_large_data_csv(\"./data2/\",temperature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We have created 86 chunks of Temperature data\n"
     ]
    }
   ],
   "source": [
    "csv2_chunks=fileCount(\"./data2/\",\"csv\")\n",
    "print(\" We have created \"+ str(csv2_chunks) +\" chunks of Temperature data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temperature_data1 = pd.read_csv('./data2/chunk1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first 5 rows of df_temp\n",
    "df_temperature_data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL, DataFrames and Datasets\n",
    "Spark SQL is a Spark module for structured data processing. \n",
    "\n",
    "Internally, Spark SQL uses this extra information to perform extra optimizations.\n",
    "\n",
    "There are several ways to interact with Spark SQL including SQL and the Dataset API. \n",
    "\n",
    "\n",
    "\n",
    "SQL\n",
    "One use of Spark SQL is to execute SQL queries. Spark SQL can also be used to read data from an existing Hive installation.\n",
    "When running SQL from within another programming language the results will be returned as a Dataset/DataFrame.\n",
    "\n",
    "\n",
    "#### Datasets vs DataFrames\n",
    "A **Dataset** is a distributed collection of data. Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. \n",
    "\n",
    "A Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.). The Dataset API is available in Scala and Java. \n",
    "\n",
    "Python does not have the support for the Dataset API. But due to Python’s dynamic nature, many of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally row.columnName).\n",
    "\n",
    "A **DataFrame** is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood.\n",
    "\n",
    "DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows.\n",
    "In the Scala API, DataFrame is simply a type alias of Dataset[Row]. While, in Java API, users need to use Dataset<Row> to represent a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We install the librarires needed\n",
    "#pip install py4j\n",
    "#pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we load only one chunk of the Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: integer (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark1 = SparkSession.builder.appName('Ops').getOrCreate()\n",
    "dfspark1=spark1.read.csv('./data/chunk1.csv',inferSchema=True,header=True)\n",
    "###Print the Schema\n",
    "dfspark1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we load only one chunk of the Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark2 = SparkSession.builder.appName('Ops').getOrCreate()\n",
    "dfspark2=spark2.read.csv('./data2/chunk1.csv',inferSchema=True,header=True)\n",
    "###Print the Schema\n",
    "dfspark2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "\n",
    "We remove the non valid data from the  I94 immigration dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of valid i94port codes is created\n",
    "re_obj = re.compile(r'\\'(.*)\\'.*\\'(.*)\\'')\n",
    "i94portvalid = {}\n",
    "with open('i94port.txt') as f:\n",
    "     for data in f:\n",
    "         match = re_obj.search(data)\n",
    "         i94portvalid[match[1]]=[match[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean immigration data full dataset\n",
    "def clean_immigration_data(file):\n",
    "    '''    \n",
    "    Input: Path to immigration data file\n",
    "    Output: Spark dataframe of immigration data with valid i94port\n",
    "    '''    \n",
    "    # Read I94 data into Spark\n",
    "    df_immigration = spark.read.format('com.github.saurfang.sas.spark').load(file)\n",
    "    # Filter out entries where i94port is invalid\n",
    "    df_immigration = df_immigration.filter(df_immigration.i94port.isin(list(i94portvalid.keys())))\n",
    "    return df_immigration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning single chunk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inmigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean immigration data\n",
    "def clean_immigration_data1(file):\n",
    "    '''    \n",
    "    Input: Path to immigration data file\n",
    "    Output: Spark dataframe of immigration data with valid i94port\n",
    "    '''    \n",
    "    # Read I94 data into Spark\n",
    "    spark1 = SparkSession.builder.appName('Ops').getOrCreate()\n",
    "    df_immigration=spark1.read.csv(file,inferSchema=True,header=True)\n",
    "    # Filter out entries where i94port is invalid\n",
    "    df_immigration = df_immigration.filter(df_immigration.i94port.isin(list(i94portvalid.keys())))\n",
    "    return df_immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(df_immigration_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "immigration_test_file = './data/chunk1.csv' \n",
    "df_immigration_test = clean_immigration_data1(immigration_test_file)\n",
    "#df_immigration_test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "We remove the data points where AverageTemperature is NaN, duplicate locations, and we include the i94port of the location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean temperature data\n",
    "df_temperature_data1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../../data2/GlobalLandTemperaturesByCity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temperature_data1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"./data2/chunk1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temperature_data1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out data points with NaN average temperature\n",
    "df_temperature_data1 = df_temperature_data1.filter(df_temperature_data1.AverageTemperature != 'NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temperature_data1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We define the key port\n",
    "@udf()\n",
    "def get_i94port(city):\n",
    "    for key in i94portvalid:\n",
    "        if city.lower() in i94portvalid[key][0].lower():\n",
    "            return key\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add iport94 code based on city name\n",
    "df_temperature_data1 = df_temperature_data1.withColumn(\"i94port\", get_i94port(df_temperature_data1.City))\n",
    "#df_temperature_data1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to Extract First N rows in pyspark we will be using functions like show() function and head() function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(dt='1743-11-01', AverageTemperature='6.068', AverageTemperatureUncertainty='1.7369999999999999', City='Århus', Country='Denmark', Latitude='57.05N', Longitude='10.33E', i94port=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract First row of dataframe in pyspark – using first() function.\n",
    "########## Extract first row of the dataframe in pyspark\n",
    "df_temperature_data1.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+-------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|i94port|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+-------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "|1744-04-01|5.7879999999999985|           3.6239999999999997|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "|1744-05-01|            10.644|           1.2830000000000001|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "|1744-06-01|14.050999999999998|                        1.347|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "|1744-07-01|            16.082|                        1.396|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########## Extract first N row of the dataframe in pyspark – show()\n",
    "df_temperature_data1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Extract first N row of the dataframe in pyspark – head()\n",
    "#df_temperature_data1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Extract first N row of the dataframe in pyspark – take()\n",
    "#df_temperature_data1.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Rows with NULL Values on Selected Columns\n",
    "In order to remove Rows with NULL values on selected columns of PySpark DataFrame, use drop(columns:Seq[String]) or drop(columns:Array[String]). To these functions pass the names of the columns you wanted to check for NULL values to delete rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_temperature_data_drop=df_temperature_data1.na.drop(subset=[\"i94port\"]) \\\n",
    "#   .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temperature_data_drop=df_temperature_data1.na.drop(subset=[\"i94port\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temperature_data_drop.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data points with no iport94 code\n",
    "df_temperature_data1 = df_temperature_data1.filter(df_temperature_data1.i94port != 'null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show results\n",
    "#df_temperature_data1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean immigration data\n",
    "def clean_temperature_data1(file):\n",
    "    df_temperature_data1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(file)\n",
    "    # Filter out data points with NaN average temperature\n",
    "    df_temperature_data1 = df_temperature_data1.filter(df_temperature_data1.AverageTemperature != 'NaN')\n",
    "    # Add iport94 code based on city name\n",
    "    df_temperature_data1 = df_temperature_data1.withColumn(\"i94port\", get_i94port(df_temperature_data1.City))\n",
    "    # Remove data points with no iport94 code\n",
    "    df_temperature_data1 = df_temperature_data1.filter(df_temperature_data1.i94port != 'null')\n",
    "    return df_temperature_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path2=\"./data2/chunk1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temperature_data1b=clean_temperature_data1(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results\n",
    "#df_temperature_data1b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(dt,StringType,true),StructField(AverageTemperature,StringType,true),StructField(AverageTemperatureUncertainty,StringType,true),StructField(City,StringType,true),StructField(Country,StringType,true),StructField(Latitude,StringType,true),StructField(Longitude,StringType,true),StructField(i94port,StringType,true)))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temperature_data1b.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean multiple chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clean the multiple chunks by cleaning single chunks iteratively and combine them into a big dataframe in pyspark.\n",
    "\n",
    "PySpark provides multiple ways to combine dataframes i.e. join, merge, union, SQL interface, etc. \n",
    "\n",
    "\n",
    "We will take a look at how the PySpark join function is similar to SQL join, where two or more tables or dataframes can be combined based on conditions. \n",
    "\n",
    "Let's take a look at some of the join operations supported by PySpark .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we choose two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration_test1=clean_immigration_data1('./data/chunk1.csv')\n",
    "df_immigration_test2=clean_immigration_data1('./data/chunk2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99905, 28)\n",
      "(99891, 28)\n"
     ]
    }
   ],
   "source": [
    "print((df_immigration_test1.count(), len(df_immigration_test1.columns)))\n",
    "print((df_immigration_test2.count(), len(df_immigration_test2.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometime, when the dataframes to combine do not have the same order of columns, it is better to df2.select(df1.columns) in order to ensure both df have the same column order before the union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools \n",
    "def unionAll(dfs):\n",
    "    return functools.reduce(lambda df1,df2: df1.union(df2.select(df1.columns)), dfs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "unioned_df = unionAll([df_immigration_test1,df_immigration_test2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199796, 28)\n"
     ]
    }
   ],
   "source": [
    "print((unioned_df.count(), len(unioned_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which is the method that we will consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition if all df have the same column order before the union we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce  # For Python 3.x\n",
    "from pyspark.sql import DataFrame\n",
    "def unionAllsame(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "unioned_df_same = unionAllsame([df_immigration_test1,df_immigration_test2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However for this project he prefer *unionAll* definition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Multiple Chunks for Data Immigration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a function that create a big dataframe in Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For lack of time just I presnet the firsts chunks. In a real project we should consider all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_large_data_sas(path):\n",
    "# We create clean alll the chunks of the big dataset\n",
    "# path : path where I want save the chunks\n",
    "    path_name=path+'chunk'\n",
    "    csv_chunks=fileCount(\"./data/\",\"csv\")\n",
    "    chunk_size=2#csv_chunks\n",
    "    batch_no=1\n",
    "     # We create  an empty df1 Pyspark Dataframe with schema            \n",
    "    schemas=clean_immigration_data1(path_name+str(batch_no)+'.csv').schema\n",
    "    df1 = spark.createDataFrame([], schemas)\n",
    "    \n",
    "    for chunk in range(chunk_size):\n",
    "        file=path_name+str(batch_no)+'.csv'\n",
    "        #print(file)\n",
    "        df_immigration_chunk= clean_immigration_data1(file)\n",
    "        #print((df_immigration_chunk.count(), len(df_immigration_chunk.columns)))\n",
    "        df1 = unionAll([df1,df_immigration_chunk])\n",
    "       # df_immigration_chunk.show()\n",
    "        #df1.show()\n",
    "        #print((df1.count(), len(df1.columns)))\n",
    "        batch_no+=1\n",
    "    return df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_immigration_all=clean_large_data_sas(\"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_immigration_all.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Multiple Chunks for Temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_large_data_csv(path):\n",
    "# We create clean alll the chunks of the big dataset\n",
    "# path : path where I want save the chunks\n",
    "    path_name=path+'chunk'\n",
    "    csv_chunks=fileCount(\"./data2/\",\"csv\")\n",
    "    chunk_size=2 #csv_chunks\n",
    "    batch_no=1\n",
    "     # We create  an empty df1 Pyspark Dataframe with schema            \n",
    "    schemas=clean_temperature_data1(path_name+str(batch_no)+'.csv').schema\n",
    "    df1 = spark.createDataFrame([], schemas)\n",
    "    for chunk in range(chunk_size):\n",
    "        file=path_name+str(batch_no)+'.csv'\n",
    "        #print(file)\n",
    "        df_temperature_data1=clean_temperature_data1(file)\n",
    "        df1 = unionAll([df1,df_temperature_data1])\n",
    "        #df1.show()\n",
    "        #print((df1.count(), len(df1.columns)))\n",
    "        batch_no+=1\n",
    "    return df1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temperature_all=clean_large_data_csv(\"./data2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temperature_all.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "\n",
    "#### 3.1 Conceptual Data Model\n",
    "**Result Table** - I94 immigration data joined with the city temperature data on i94port,\n",
    "Columns:\n",
    "   - i94yr = 4 digit year,\n",
    "   - i94mon = numeric month,\n",
    "   - i94cit = 3 digit code of origin city,\n",
    "   - i94port = 3 character code of destination USA city,\n",
    "   - arrdate = arrival date in the USA,\n",
    "   - i94mode = 1 digit travel code,\n",
    "   - depdate = departure date from the USA,\n",
    "   - i94visa = reason for immigration,\n",
    "   - AverageTemperature = average temperature of destination city,\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\\n\n",
    "\n",
    "Pipeline Steps:\n",
    "\n",
    "1. We define the paths of the source files of immigration data.\n",
    "2. Split data of the I94 immigration data into chuncks\n",
    "3. We have to clean the data of the I94 dataset of all chunks \n",
    "4. We extract columns that are useful for the analsyis\n",
    "5. Optionally we create a paquet file of the immigration data partitioned by i94port.\n",
    "6. Split data of the temperautre  data into chuncks\n",
    "7. Clean the temperatures dataset and we create a  Spark dataframe df_temperature_data.\n",
    "8. Optionally we can create a parquet file of the temperatures table partitioned by i94port.\n",
    "9. Create result table by merging immigration and temperature tables\n",
    "10. We write the results table in  parquet file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- Path to I94 immigration data\n",
    "#immigration_data = '/data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-Split data\n",
    "#split_large_data_sas(\"../../data/\",immigration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-Clean I94 immigration data and store as Spark dataframe\n",
    "#df_immigration_all=clean_large_data_sas(\"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration=df_immigration_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4-  Extract columns for immigration \n",
    "immigration_table = df_immigration.select([\"i94yr\", \"i94mon\", \"i94cit\", \"i94port\", \"arrdate\", \"i94mode\", \"depdate\", \"i94visa\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columnar storage like Apache Parquet is designed to bring efficiency compared to row-based files like CSV. When querying, columnar storage you can skip over the non-relevant data very quickly. As a result, aggregation queries are less time consuming compared to row-oriented databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5- Write immigration table to parquet files partitioned by i94port\n",
    "#immigration_table.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"./data/immigration.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6Read in the temperature data\n",
    "#temperature_data = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "#split_large_data_csv(\"./data2/\",temperature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temperature_all=clean_large_data_csv(\"./data2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temperature_data1=df_temperature_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 Extract columns for temperature  table\n",
    "temp_table = df_temperature_data1.select([\"AverageTemperature\", \"City\", \"Country\", \"Latitude\", \"Longitude\", \"i94port\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8 Write temperature dimension table to parquet files partitioned by i94port\n",
    "#temp_table.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"./data2/temperature.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We create table of results of the immigration and temperature data\n",
    "df_immigration.createOrReplaceTempView(\"immigration_view\")\n",
    "df_temperature_data1.createOrReplaceTempView(\"temperature_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 We werge the information of temperature and immigration\n",
    "result_table = spark.sql('''\n",
    "select immigration_view.i94yr as year,\n",
    "       immigration_view.i94mon as month,\n",
    "       immigration_view.i94cit as city,\n",
    "       immigration_view.i94port as i94port,\n",
    "       immigration_view.arrdate as arrival_date,\n",
    "       immigration_view.depdate as departure_date,\n",
    "       immigration_view.i94visa as reason,\n",
    "       temperature_view.AverageTemperature as temperature,\n",
    "       temperature_view.Latitude as latitude,\n",
    "       temperature_view.Longitude as longitude\n",
    "from immigration_view\n",
    "JOIN temperature_view ON (immigration_view.i94port = temperature_view.i94port)\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 improve the writting of parquet\n",
    "n = 2 # number of repartitions, try 2 to test\n",
    "path=\"./results/result.parquet\"\n",
    "spark_df  = result_table.repartition(n)\n",
    "spark_df.write.mode(\"overwrite\").partitionBy(\"i94port\").parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_check(df, description):\n",
    "    '''\n",
    "    Input: Spark dataframe, description of Spark datafram\n",
    "    Output: Print outcome of data quality check\n",
    "    '''\n",
    "    \n",
    "    result = df.count()\n",
    "    if result == 0:\n",
    "        print(\"Data quality check failed for {} with zero records\".format(description))\n",
    "    else:\n",
    "        print(\"Data quality check passed for {} with {} records\".format(description, result))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data quality check\n",
    "#quality_check(df_immigration, \"immigration table\")\n",
    "#quality_check(df_temperature_data1, \"temperature table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data quality check passed for immigration table with 3088544 records\n",
    "Data quality check passed for temperature table with 539191 records\n",
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "**Results Table** - \n",
    "Columns:\n",
    "   - i94yr = 4 digit year,\n",
    "   - i94mon = numeric month,\n",
    "   - i94cit = 3 digit code of origin city,\n",
    "   - i94port = 3 character code of destination USA city,\n",
    "   - arrdate = arrival date in the USA,\n",
    "   - i94mode = 1 digit travel code,\n",
    "   - depdate = departure date from the USA,\n",
    "   - i94visa = reason for immigration,\n",
    "   - AverageTemperature = average temperature of destination city,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis  of the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Actions:\n",
    "spark dataframe does not contain data, it contains instructions and operation graph, since spark works with big data it does not allow to perform any operation as its called, to prevent slow performance, instead, methods are separated into two kinds Actions and Transformations, transformations are collected and contained as an operation graph.\n",
    "\n",
    "Action is a method that causes the dataframe to execute all accumulated operation in the graph, causing the slow performances, since it execute everything (note, UDFs are extremely slow).\n",
    "\n",
    "show() is an action, when you call show() it has to calculate every other transformation to show you the true data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We want to analyze the database to see the immigration behaviour to location temperatures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading The Results in Parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# initialise sparkContext\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local') \\\n",
    "    .appName('myAppName') \\\n",
    "    .config('spark.executor.memory', '5gb') \\\n",
    "    .config(\"spark.cores.max\", \"6\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# using SQLContext to read parquet file\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: double, month: double, city: double, i94port: string, arrival_date: double, departure_date: double, reason: double, temperature: string, latitude: string, longitude: string]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(False, 0.1, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: double (nullable = true)\n",
      " |-- month: double (nullable = true)\n",
      " |-- city: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrival_date: double (nullable = true)\n",
      " |-- departure_date: double (nullable = true)\n",
      " |-- reason: double (nullable = true)\n",
      " |-- temperature: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to read parquet file\n",
    "df = sqlContext.read.parquet('./results/result.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: double (nullable = true)\n",
      " |-- month: double (nullable = true)\n",
      " |-- city: double (nullable = true)\n",
      " |-- arrival_date: double (nullable = true)\n",
      " |-- departure_date: double (nullable = true)\n",
      " |-- reason: double (nullable = true)\n",
      " |-- temperature: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas and plot\n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>city</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>departure_date</th>\n",
       "      <th>reason</th>\n",
       "      <th>temperature</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>i94port</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>20546.0</td>\n",
       "      <td>20555.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>31.88300000000001</td>\n",
       "      <td>24.92N</td>\n",
       "      <td>54.98E</td>\n",
       "      <td>MAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>20557.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.230999999999998</td>\n",
       "      <td>24.92N</td>\n",
       "      <td>54.98E</td>\n",
       "      <td>MAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>20557.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.230999999999998</td>\n",
       "      <td>24.92N</td>\n",
       "      <td>54.98E</td>\n",
       "      <td>MAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>20639.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.089</td>\n",
       "      <td>24.92N</td>\n",
       "      <td>54.98E</td>\n",
       "      <td>MAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.679</td>\n",
       "      <td>24.92N</td>\n",
       "      <td>54.98E</td>\n",
       "      <td>MAA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     year  month   city  arrival_date  departure_date  reason  \\\n",
       "0  2016.0    4.0  274.0       20546.0         20555.0     2.0   \n",
       "1  2016.0    4.0  213.0       20545.0         20557.0     1.0   \n",
       "2  2016.0    4.0  213.0       20545.0         20557.0     1.0   \n",
       "3  2016.0    4.0  213.0       20545.0         20639.0     2.0   \n",
       "4  2016.0    4.0  244.0       20545.0             NaN     3.0   \n",
       "\n",
       "          temperature latitude longitude i94port  \n",
       "0   31.88300000000001   24.92N    54.98E     MAA  \n",
       "1  17.230999999999998   24.92N    54.98E     MAA  \n",
       "2  17.230999999999998   24.92N    54.98E     MAA  \n",
       "3              30.089   24.92N    54.98E     MAA  \n",
       "4              19.679   24.92N    54.98E     MAA  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = pdf.columns.drop('i94port')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf[cols] = pdf[cols].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7efc84e8f630>]], dtype=object)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efccf8104a8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pdf.hist(column='temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7efc84e86a58>]], dtype=object)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efc84eb2dd8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pdf.hist(column='city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_city=pdf.loc[(pdf['city'] <= 300)& (pdf['temperature'] >= 25) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7efc85719198>]], dtype=object)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efc856a89b0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pdf_city.hist(column='temperature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means that there is more inmmigration cases to the cities where the temperature as around 32 to 34 degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project:\n",
    "\n",
    "The strategy of this project depends on the **amount of data** that you have to process in your ETL.\n",
    "In this project I assume that the amount of that is relly big and the amount of resources in **memory** is limited.\n",
    "\n",
    "\n",
    "1. In ordering to del with this great amount of that we should split in chunks.\n",
    "\n",
    "<img src=\"sharding-range-based.bakedsvg.svg\" width=\"400\">\n",
    "\n",
    "\n",
    "\n",
    "2. In ordring to have performance in the large scale data processing we use pache Spark\n",
    "\n",
    "<img src=\"largest-open-source-apache-spark.png\" width=\"400\">\n",
    "\n",
    "a) Engineered from the bottom-up for performance, Spark can be 100x faster than Hadoop for large scale data processing by exploiting in memory computing and other optimizations. Spark is also fast when data is stored on disk, and currently holds the world record for large-scale on-disk sorting.\n",
    "\n",
    "b) Spark has easy-to-use APIs for operating on large datasets. This includes a collection of over 100 operators for transforming data and familiar data frame APIs for manipulating semi-structured data.\n",
    "\n",
    "c) Spark comes packaged with higher-level libraries, including support for SQL queries, streaming data, machine learning and graph processing. These standard libraries increase developer productivity and can be seamlessly combined to create complex workflows.\n",
    "\n",
    "d) In additon Spark can andle multiple file formats (SAS, csv, etc) that contain large amounts of data.  Spark SQL was used to process the input files into dataframes and manipulated via standard SQL join operations to create the tables.\n",
    "\n",
    "\n",
    "\n",
    "### Scenarios\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    "\n",
    "\n",
    "1. We can use this pipeline when the amount of that is bigger than 100Gb\n",
    "     The AWS service  we can use is  Amazon Redshift: It is an analytical database that is optimized for aggregation and read-heavy workloads\n",
    "\n",
    "\n",
    "\n",
    "2. The data populates a dashboard that must be updated on a daily basis by 6 am every day before people people wake up.\n",
    "  \n",
    "  \n",
    "3.  Apache Airflow is a scheduler to manage your regular jobs. It is an excellent tool to organize, execute, and monitor your workflows so that they work seamlessly. Apache Airflow solved a lot of problems that the predecessors faced.\n",
    "  \n",
    "4.  So we can use Ariflow to create DAG retries or send emails on failures and perform daily quality checks; if fail, send emails to operators and freeze dashboards\n",
    "\n",
    "\n",
    "5. The database needed to be accessed by many people.\n",
    "    Amazon Redshift makes it easy to add nodes to your data warehouse and enables you to maintain fast query performance as your data warehouse grows. Redshift Spectrum: Redshift Spectrum enables you to run queries against exabytes of data in Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
